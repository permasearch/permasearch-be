In some cases it is possible to achieve a good correspondence between      X  b     {\displaystyle X_{b}}   and      Y  b     {\displaystyle Y_{b}}   with numbers as low as      n  x   = 2   {\displaystyle n_{x}=2}   and      n  y   = 2   {\displaystyle n_{y}=2}  , while in other cases the number of bins required may be higher. The maximum for      I  (  X  b   ;  Y  b   )   {\displaystyle \mathrm {I} (X_{b};Y_{b})}   is determined by H(X), which is in turn determined by the number of bins in each axis, therefore, the mutual information value will be dependent on the number of bins selected for each variable. In order to compare mutual information values obtained with partitions of different sizes, the mutual information value is normalized by dividing by the maximum achieveable value for the given partition size. It is worth noting that a similar adaptive binning procedure for estimating mutual information had been proposed previously[10]. Entropy is maximized by uniform probability distributions, or in this case, bins with the same number of elements. Also, joint entropy is minimized by having a one-to-one correspondence between bins. If we substitute such values in the formula     I ( X ; Y ) = H ( X ) + H ( Y ) − H ( X , Y )   {\displaystyle I(X;Y)=H(X)+H(Y)-H(X,Y)}  , we can see that the maximum value achieveable by the MI for a given pair      n  x   ,  n  y     {\displaystyle n_{x},n_{y}}   of bin counts is     log ⁡ min  (  n  x   ,  n  y   )    {\displaystyle \log \min \left(n_{x},n_{y}\right)}  . Thus, this value is used as a normalizing divisor for each pair of bin counts.