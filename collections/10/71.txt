Originally, the Uniform Guidelines on Employee Selection Procedures provided a simple "80 percent" rule for determining that a company's selection system was having an "adverse impact" on a minority group. The rule was based on the rates at which job applicants were hired. For example, if XYZ Company hired 50 percent of the men applying for work in a predominantly male occupation while hiring only 20 percent of the female applicants, one could look at the ratio of those two hiring rates to judge whether there might be a discrimination problem. The ratio of 20:50 means that the rate of hiring for female applicants is only 40 percent of the rate of hiring for male applicants. That is, 20 divided by 50 equals 0.40, which is equivalent to 40 percent. Clearly, 40 percent is well below the 80 percent that was arbitrarily set as an acceptable difference in hiring rates. Therefore, in this example, XYZ Company could have been called upon to prove that there was a legitimate reason for hiring men at a rate so much higher than the rate of hiring women. Since the 1980s, courts in the U.S. have questioned the arbitrary nature of the 80 percent rule, making the rule less important than it was when the Uniform Guidelines were first published. A recent memorandum from the U.S. Equal Employment Opportunities Commission suggests that a more defensible standard would be based on comparing a company's hiring rate of a particular group with the rate that would occur if the company simply selected people at random.[11] In other words, if a company's selection system made it statistically more difficult than pure chance for a member of a certain group, such as women or African-Americans, to get a job, then this could be reasonably viewed as evidence that the selection system was systematically screening out members of that social group.