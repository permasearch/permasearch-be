The mutual information of a distribution is a special case of the Kullback–Leibler divergence in which     P   {\displaystyle P}   is the full multivariate distribution and     Q   {\displaystyle Q}   is the product of the 1-dimensional marginal distributions. In the notation of the Kullback–Leibler divergence section of this article,       Σ   1     {\displaystyle {\boldsymbol {\Sigma }}_{1}}   is a diagonal matrix with the diagonal entries of       Σ   0     {\displaystyle {\boldsymbol {\Sigma }}_{0}}  , and       μ   1   =   μ   0     {\displaystyle {\boldsymbol {\mu }}_{1}={\boldsymbol {\mu }}_{0}}  . The resulting formula for mutual information is: