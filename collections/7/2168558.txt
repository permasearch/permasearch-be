On the other hand, one can check that the target function for the hinge loss is exactly      f  ∗     {\displaystyle f^{*}}  . Thus, in a sufficiently rich hypothesis space—or equivalently, for an appropriately chosen kernel—the SVM classifier will converge to the simplest function (in terms of       R     {\displaystyle {\mathcal {R}}}  ) that correctly classifies the data. This extends the geometric interpretation of SVM—for linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier.[18]