Using        y ^    = X    β ^      {\displaystyle {\hat {y}}=X{\hat {\beta }}}   in this, and simplifying to obtain         y ^     T      y ^    =  y  T   X (  X  T   X  )  − 1    X  T   y   {\displaystyle {\hat {y}}^{T}{\hat {y}}=y^{T}X(X^{T}X)^{-1}X^{T}y}  , gives the result that TSS = ESS + RSS if and only if      y  T      y ¯    =     y ^     T      y ¯      {\displaystyle y^{T}{\bar {y}}={\hat {y}}^{T}{\bar {y}}}  . The left side of this is      y  m     {\displaystyle y_{m}}   times the sum of the elements of y, and the right side is      y  m     {\displaystyle y_{m}}   times the sum of the elements of        y ^      {\displaystyle {\hat {y}}}  , so the condition is that the sum of the elements of y equals the sum of the elements of        y ^      {\displaystyle {\hat {y}}}  , or equivalently that the sum of the prediction errors (residuals)      y  i   −     y ^     i     {\displaystyle y_{i}-{\hat {y}}_{i}}   is zero. This can be seen to be true by noting the well-known OLS property that the k × 1 vector      X  T      e ^    =  X  T   [ I − X (  X  T   X  )  − 1    X  T   ] y = 0   {\displaystyle X^{T}{\hat {e}}=X^{T}[I-X(X^{T}X)^{-1}X^{T}]y=0}  : since the first column of X is a vector of ones, the first element of this vector      X  T      e ^      {\displaystyle X^{T}{\hat {e}}}   is the sum of the residuals and is equal to zero. This proves that the condition holds for the result that TSS = ESS + RSS.