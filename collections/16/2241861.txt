Now let     θ ( x ) ∈   R    2  N       {\displaystyle \theta (x)\in \mathbb {R} ^{2^{N}}}   be the vector of excesses of     x   {\displaystyle x}  , arranged in non-increasing order. In other words,      θ  i   ( x ) ≥  θ  j   ( x ) , ∀   i < j   {\displaystyle \theta _{i}(x)\geq \theta _{j}(x),\forall ~i<j}  . Notice that     x   {\displaystyle x}   is in the core of     v   {\displaystyle v}   if and only if it is a pre-imputation and      θ  1   ( x ) ≤ 0   {\displaystyle \theta _{1}(x)\leq 0}  . To define the nucleolus, we consider the lexicographic ordering of vectors in       R    2  N       {\displaystyle \mathbb {R} ^{2^{N}}}  : For two payoff vectors     x , y   {\displaystyle x,y}  , we say     θ ( x )   {\displaystyle \theta (x)}   is lexicographically smaller than     θ ( y )   {\displaystyle \theta (y)}   if for some index     k   {\displaystyle k}  , we have      θ  i   ( x ) =  θ  i   ( y ) , ∀   i < k   {\displaystyle \theta _{i}(x)=\theta _{i}(y),\forall ~i<k}   and      θ  k   ( x ) <  θ  k   ( y )   {\displaystyle \theta _{k}(x)<\theta _{k}(y)}  . (The ordering is called lexicographic because it mimics alphabetical ordering used to arrange words in a dictionary.) The nucleolus of     v   {\displaystyle v}   is the lexicographically minimal imputation, based on this ordering. This solution concept was first introduced in (Schmeidler 1969).